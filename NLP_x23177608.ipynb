{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "808ebf9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/dia.srivastava.12/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "import glob\n",
    "import nltk\n",
    "nltk.download('popular');\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import multiprocessing \n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6aebb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stop = stopwords.words('english') + list(string.punctuation) + ['\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "25080427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_path(fld_path):                             #give path of the folder containing all documents\n",
    "    dic = {}\n",
    "    file_names = glob.glob(fld_path)\n",
    "    files_150 = file_names[0:10]\n",
    "    for file in files_150:\n",
    "        name = file.split('/')[-1]\n",
    "        with open(file, 'r', errors='ignore') as f:\n",
    "            data = f.read()\n",
    "        dic[name] = data\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f8b9e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordList_removePuncs(doc_dict):\n",
    "\n",
    "    wordList = []\n",
    "    words_in_doc = {}\n",
    "    for doc_id, doc in doc_dict.items():\n",
    "        for word in word_tokenize(doc.lower().strip()):  # Tokenize the text into words\n",
    "            if word not in Stop:  # Exclude stopwords and punctuation\n",
    "                wordList.append(word)  # Add word to list\n",
    "        words_in_doc[doc_id] = len(wordList)  # Store number of words in the document\n",
    "    return wordList, words_in_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ad5ae07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFrequencyInDoc(vocab, doc_dict):\n",
    "    tf_docs = {}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        tf_docs[doc_id] = {}\n",
    "    \n",
    "    for word in vocab:\n",
    "        for doc_id,doc in doc_dict.items():\n",
    "            tf_docs[doc_id][word] = doc.count(word)\n",
    "    return tf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "438324f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logNorm_tf(tf_doc):\n",
    "    ln_tf={}\n",
    "    for doc_id, doc in tf_doc.items():\n",
    "        ln_tf[doc_id] = {word: 1 + np.log(tf) if tf > 0 else 0 for word, tf in doc.items()}\n",
    "    return ln_tf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "88f49549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordDocFre(vocab, doc_dict):\n",
    "    df = {word: 0 for word in vocab}\n",
    "    \n",
    "    for doc in doc_dict.values():\n",
    "        words = set(word_tokenize(doc.lower().strip()))\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                df[word] += 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "7604a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverseDocFre(vocab,doc_fre,length):\n",
    "    idf= {} \n",
    "    for word in vocab:     \n",
    "        idf[word] = np.log2((length+1) / doc_fre[word])\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "06ca20b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(vocab,tf,idf_scr,doc_dict):\n",
    "    tf_idf_scr = {}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        tf_idf_scr[doc_id] = {}\n",
    "    for word in vocab:\n",
    "        for doc_id,doc in doc_dict.items():\n",
    "            tf_idf_scr[doc_id][word] = tf[doc_id][word] * idf_scr[word]\n",
    "    return tf_idf_scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "6e8c5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorSpaceModel(query, doc_dict,tfidf_scr):\n",
    "    query_vocab = []\n",
    "    for word in query.split():\n",
    "        if word not in query_vocab:\n",
    "            query_vocab.append(word)\n",
    "    query_wc = {}\n",
    "    for word in query_vocab:\n",
    "        query_wc[word] = query.lower().split().count(word)\n",
    "        relevance_scores = {}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        score = 0\n",
    "        for word in query_vocab:\n",
    "            score += query_wc[word] * tfidf_scr[doc_id][word]\n",
    "        relevance_scores[doc_id] = score\n",
    "    sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))\n",
    "    top_5 = {k: sorted_value[k] for k in list(sorted_value)[:5]}\n",
    "    return top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "4d23798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_norm_words(docs, words_in_docs, tf_dict):\n",
    "    tf_norm = {\n",
    "        doc_id: {\n",
    "            word: tf / words_in_docs[doc_id] for word, tf in tf_dict[doc_id].items()\n",
    "        } for doc_id in docs.keys()\n",
    "    }\n",
    "    return tf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0504b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6c3986f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents Fetched: dict_keys(['W09-3025.pdf.txt', 'P87-1010.pdf.txt', 'W14-5314.pdf.txt', 'J13-4002.pdf.txt', 'W98-1306.pdf.txt', 'W00-1324.pdf.txt', 'W08-2121.pdf.txt', 'P09-4008.pdf.txt', 'P14-1053.pdf.txt', 'P84-1066.pdf.txt'])\n",
      "Fetching files: 10\n",
      "Number of Docs: 10\n",
      "Calculating TF-IDF scores... without normalization\n",
      "TF-IDF scores calculated without normalization\n",
      "Calculating TF-IDF scores... with number of words in a doc normalization\n",
      "TF-IDF scores calculated with number of words in a doc normalization\n",
      "Calculating TF-IDF scores... with log normalization\n",
      "TF-IDF scores calculated with log normalization\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'J13-4002.pdf.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[244], line 42\u001b[0m\n\u001b[1;32m     33\u001b[0m queries\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLDA\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerative models\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     39\u001b[0m ]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[0;32m---> 42\u001b[0m     top_documents \u001b[38;5;241m=\u001b[39m vectorSpaceModel(query, docs, tf_idf)\n\u001b[1;32m     43\u001b[0m     top_documents_norm \u001b[38;5;241m=\u001b[39m vectorSpaceModel(query, docs, tf_idf_norm)\n\u001b[1;32m     44\u001b[0m     top_documents_log \u001b[38;5;241m=\u001b[39m vectorSpaceModel(query, docs, tf_idf_log)\n",
      "Cell \u001b[0;32mIn[241], line 13\u001b[0m, in \u001b[0;36mvectorSpaceModel\u001b[0;34m(query, doc_dict, tfidf_scr)\u001b[0m\n\u001b[1;32m     11\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m query_vocab:\n\u001b[0;32m---> 13\u001b[0m         score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m query_wc[word] \u001b[38;5;241m*\u001b[39m tfidf_scr[doc_id][word]\n\u001b[1;32m     14\u001b[0m     relevance_scores[doc_id] \u001b[38;5;241m=\u001b[39m score\n\u001b[1;32m     15\u001b[0m sorted_value \u001b[38;5;241m=\u001b[39m OrderedDict(\u001b[38;5;28msorted\u001b[39m(relevance_scores\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'J13-4002.pdf.txt'"
     ]
    }
   ],
   "source": [
    "if __name__  == \"__main__\":\n",
    "    path = 'ACL/*.txt'\n",
    "    docs = give_path(path)                        #returns a dictionary of all docs\n",
    "    print('Documents Fetched:', docs.keys())\n",
    "    print('Fetching files: 10')\n",
    "    \n",
    "    M = len(docs) \n",
    "      #number of files in dataset\n",
    "    w_List, words_in_docs = wordList_removePuncs(docs)           #returns a list of tokenized words\n",
    "    vocab = list(set(w_List))                     #returns a list of unique words\n",
    "    print('Number of Docs:', M)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Calculating TF-IDF scores... without normalization')\n",
    "    tf_dict = termFrequencyInDoc(vocab, docs)     #returns term frequency\n",
    "    df_dict = wordDocFre(vocab, docs)             #returns document frequencies\n",
    "    idf_dict = inverseDocFre(vocab,df_dict,M)     #returns idf scores\n",
    "    print('TF-IDF scores calculated without normalization')\n",
    "    \n",
    "    \n",
    "    print('Calculating TF-IDF scores... with number of words in a doc normalization')\n",
    "    tf_norm = tf_norm_words(docs, words_in_docs, tf_dict)\n",
    "    tf_idf_norm = tfidf(vocab, tf_norm, idf_dict, docs)  #returns tf-idf socres\n",
    "    print('TF-IDF scores calculated with number of words in a doc normalization')\n",
    "    \n",
    "    \n",
    "    print('Calculating TF-IDF scores... with log normalization')\n",
    "    tf_log = logNorm_tf(tf_dict)\n",
    "    tf_idf_log = tfidf(vocab, tf_log, idf_dict, docs)\n",
    "    print('TF-IDF scores calculated with log normalization', end='\\n\\n')\n",
    "\n",
    "    queries=[\n",
    "        'text',\n",
    "        'LDA',\n",
    "        'topic modeling',\n",
    "        'Natural language Processing',\n",
    "        'generative models'\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        top_documents = vectorSpaceModel(query, docs, tf_idf)\n",
    "        top_documents_norm = vectorSpaceModel(query, docs, tf_idf_norm)\n",
    "        top_documents_log = vectorSpaceModel(query, docs, tf_idf_log)\n",
    "        print('\\n')\n",
    "        print('Top 5 Documents for Query \"{}\":'.format(query), end='\\n\\n')\n",
    "        print('Without Normalization:', end='\\t')\n",
    "        print(top_documents, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6db1dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without normalisation, term frequencies are not adjusted for document length, resulting in larger raw term frequencies in lengthier documents, potentially biassing comparisons.\n",
      "Longer documents may not necessarily have an advantage in terms of phrase frequency when normalised for word count.\n",
      "      It accurately represents the value of terms within papers.\n",
      "Log normalisation helps balance the relevance of terms in a document by reducing the impact of high term frequency.\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Without normalisation, term frequencies are not adjusted for document length, resulting in larger raw term frequencies in lengthier documents, potentially biassing comparisons.\n",
    "Longer documents may not necessarily have an advantage in terms of phrase frequency when normalised for word count.\n",
    "      It accurately represents the value of terms within papers.\n",
    "Log normalisation helps balance the relevance of terms in a document by reducing the impact of high term frequency.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c18ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
